{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99c51ec8",
   "metadata": {},
   "source": [
    "\n",
    "# Taller (3h) ‚Äî FastAPI + Scikit-Learn orientado a investigaci√≥n\n",
    "**Objetivo:** Dise√±ar, entrenar y servir un modelo de ML **investigando** las decisiones t√©cnicas clave.  \n",
    "**Entrega:** API funcional (FastAPI) + breve informe (markdown dentro del notebook) justificando decisiones.\n",
    "\n",
    "> Filosof√≠a del taller: menos receta, m√°s criterio. No hay una √∫nica respuesta correcta; lo evaluado es la **calidad del razonamiento**, la **limpieza de la implementaci√≥n** y la **capacidad de probar** la API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdea1218",
   "metadata": {},
   "source": [
    "\n",
    "## Agenda sugerida (3h)\n",
    "1) **Planteo del problema y dataset** (30‚Äì40 min)  \n",
    "2) **Entrenamiento y persistencia** (40‚Äì50 min)  \n",
    "3) **Dise√±o del contrato y API** (45‚Äì55 min)  \n",
    "4) **Pruebas, errores y mejoras** (30‚Äì35 min)\n",
    "\n",
    "### Reglas\n",
    "- No borres los encabezados `TODO`. Agrega tu c√≥digo debajo de cada bloque indicado.\n",
    "- Documenta tus decisiones en la secci√≥n **Bit√°cora** al final.\n",
    "- Puedes trabajar en equipo, pero cada entrega debe ser individual y original.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cc07b6",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 1) Selecci√≥n de dataset y formulaci√≥n del problema (investigaci√≥n)\n",
    "Elige **uno**:\n",
    "- A) `sklearn.datasets.load_wine` (clasificaci√≥n 3 clases, baseline r√°pido)\n",
    "- B) Un dataset tabular de UCI, Kaggle u otra fuente **citable** (debe ser peque√±o/mediano y con licencia apta)\n",
    "- C) Un CSV propio (explica origen y variables)\n",
    "\n",
    "**Requisitos m√≠nimos:**\n",
    "- Problema de **clasificaci√≥n** o **regresi√≥n** tabular\n",
    "- Al menos **6 features num√©ricas** (puedes convertir categ√≥ricas)\n",
    "- Justifica por qu√© es un buen caso para servir v√≠a API\n",
    "\n",
    "**Entrega (en esta celda, texto breve):** Describe el dataset, objetivo, variables y m√©trica principal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a51c33",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1 Carga y exploraci√≥n (TODO)\n",
    "- Carga el dataset (pd.read_csv o loader de sklearn).\n",
    "- Muestra `head()`, `describe()` y verifica nulos/outliers.\n",
    "- Selecciona `X` (features) y `y` (target); explica tu elecci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92213bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class TrainModel():\n",
    "    def load_data(self, path=\"data/Churn.xlsx\"):\n",
    "        \"\"\"Cargar dataset\"\"\"\n",
    "        df = pd.read_excel(path)\n",
    "        return df\n",
    "\n",
    "    def explore_data(self, df):\n",
    "        \"\"\"Explorar dataset.\"\"\"\n",
    "        print(\"Primeras filas:\\n\", df.head())\n",
    "        print(\"\\nInformaci√≥n del dataset:\\n\", df.describe())\n",
    "        print(\"\\nDistribuci√≥n de la variable objetivo 'Churn':\\n\", df['Churn'].value_counts(normalize=True))\n",
    "\n",
    "    def preprocess_data(self, df):\n",
    "        # Transformaci√≥n\n",
    "        df['Intl_Plan'] = df['Intl_Plan'].map({'yes': 1, 'no': 0})\n",
    "        df['Vmail_Plan'] = df['Vmail_Plan'].map({'yes': 1, 'no': 0})\n",
    "        df['Churn'] = df['Churn'].map({'True.': 1, 'False.': 0})\n",
    "\n",
    "        # Nuevas variables derivadas\n",
    "        df['Total_Calls'] = df['Day_Calls'] + df['Eve_Calls'] + df['Night_Calls'] + df['Intl_Calls']\n",
    "        df['Total_Mins'] = df['Day_Mins'] + df['Eve_Mins'] + df['Night_Mins'] + df['Intl_Mins']\n",
    "        df['Total_Charge'] = df['Day_Charge'] + df['Eve_Charge'] + df['Night_Charge'] + df['Intl_Charge']\n",
    "        df['High_Usage'] = (df['Total_Charge'] > df['Total_Charge'].mean()).astype(int)\n",
    "        df['Many_CustServ_Calls'] = (df['CustServ_Calls'] > 5).astype(int)\n",
    "\n",
    "        df = df.drop(columns=['Phone', 'State','Day_Calls', 'Eve_Calls', 'Night_Calls','Intl_Calls',   \n",
    "                                'Day_Mins', 'Eve_Mins', 'Night_Mins','Intl_Mins', \n",
    "                                'Day_Charge', 'Eve_Charge','Night_Charge', 'Intl_Charge'])\n",
    "        # Eliminar nulos\n",
    "        df = df.dropna()\n",
    "\n",
    "        X = df.drop('Churn', axis=1)\n",
    "        y = df['Churn']\n",
    "\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575da8f8",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 2) Entrenamiento y persistencia del modelo (investigaci√≥n)\n",
    "Toma decisiones y **justif√≠calas**:\n",
    "- ¬øModelo base? (p. ej. `LogisticRegression`, `RandomForest`, `XGBoost` si lo instalas)\n",
    "- ¬øPreprocesamiento? (escala, imputaci√≥n, OneHot, etc.)\n",
    "- ¬øValidaci√≥n? (`train_test_split` vs `cross_val_score`)\n",
    "- ¬øM√©trica? (clasificaci√≥n: accuracy/F1; regresi√≥n: RMSE/MAE‚Ä¶)\n",
    "\n",
    "**Requisito:** empaqueta tu flujo en un `Pipeline` de sklearn y **persiste** el modelo y columnas (joblib + JSON).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6172f286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "MODEL_PATH = \"models/trainModel.pkl\"\n",
    "\n",
    "\n",
    "class TrainModel():\n",
    "    def split_data(self, X, y, test_size=0.3, random_state=42):\n",
    "        \"\"\"Dividir dataset en train/test.\"\"\"\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def create_model_pipeline(self, max_depth=5, min_samples_leaf=10, min_samples_split=20):\n",
    "        \"\"\"Crear pipeline √°rbol de decisi√≥n.\"\"\"\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('smote', SMOTE(random_state=42)),\n",
    "            ('clf', DecisionTreeClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf,min_samples_split=min_samples_split, random_state=42))\n",
    "        ])\n",
    "        return pipeline\n",
    "\n",
    "    def train_model(self, pipeline, X_train, y_train):\n",
    "        \"\"\"Entrenar modelo de √°rbol de decisi√≥n.\"\"\"\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        return pipeline\n",
    "\n",
    "    def evaluate_model(self, model, X_test, y_test):\n",
    "        \"\"\"Evaluar modelo en test y mostrar m√©tricas.\"\"\"\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(\"\\nüìä Reporte de Clasificaci√≥n en Test:\\n\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        return y_pred\n",
    "\n",
    "    def cross_validate_model(self, model, X_train, y_train):\n",
    "        \"\"\"Cross-validation 5 folds en train.\"\"\"\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "        cv_results = cross_validate(model, X_train, y_train, cv=cv, scoring=scoring)\n",
    "        print(\"\\nüìä Resultados de cross-validation (promedio en 5 folds):\")\n",
    "        for metric in scoring:\n",
    "            print(f\"{metric}: {cv_results['test_' + metric].mean():.3f}\")\n",
    "\n",
    "    def create_model_visualization(self, model, X, y_test, y_pred):\n",
    "        \"\"\"Visualizaci√≥n del √°rbol y matriz de confusi√≥n.\"\"\"\n",
    "        plt.figure(figsize=(20, 8))\n",
    "        plot_tree(\n",
    "            model.named_steps['clf'],\n",
    "            feature_names=X.columns,\n",
    "            class_names=[\"Churn\", \"No Churn\"],\n",
    "            filled=True,\n",
    "            rounded=True,\n",
    "            fontsize=10\n",
    "        )\n",
    "        plt.title(\"√Årbol de Decisi√≥n - Churn\")\n",
    "        plt.show()\n",
    "\n",
    "        # Matriz de confusi√≥n\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Churn\", \"No Churn\"])\n",
    "        disp.plot(cmap='Blues')\n",
    "        plt.title(\"Matriz de Confusi√≥n - √Årbol de Decisi√≥n\")\n",
    "        plt.show()\n",
    "\n",
    "    def save_model(self, model, columns, path=MODEL_PATH):\n",
    "        \"\"\"Guardar modelo + columnas usadas.\"\"\"\n",
    "        joblib.dump({\n",
    "            \"model\": model,\n",
    "            \"columns\": columns\n",
    "        }, path)\n",
    "        print(f\"\\n‚úÖ Modelo guardado en {path}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Pipeline principal\n",
    "    # ------------------------------\n",
    "\n",
    "    def main(self):\n",
    "        df = self.load_data()\n",
    "        self.explore_data(df)\n",
    "        X, y = self.preprocess_data(df)\n",
    "        X_train, X_test, y_train, y_test = self.split_data(X, y)\n",
    "            \n",
    "        pipeline = self.create_model_pipeline()\n",
    "        model = self.train_model(pipeline, X_train, y_train)\n",
    "            \n",
    "        y_pred = self.evaluate_model(model, X_test, y_test)\n",
    "        self.cross_validate_model(model, X_train, y_train)\n",
    "            \n",
    "        self.create_model_visualization(model, X, y_test, y_pred)\n",
    "        self.save_model(model, X.columns.tolist())\n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "        trainer = TrainModel()\n",
    "        trainer.main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c134dd",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 3) Dise√±o del contrato de la API (investigaci√≥n)\n",
    "Define **endpoints m√≠nimos**:\n",
    "- `GET /health` (estado, versi√≥n del modelo, m√©trica)\n",
    "- `POST /predict` (un registro)\n",
    "- `POST /predict-batch` (lista de registros)\n",
    "\n",
    "**Decisiones a justificar:**\n",
    "- ¬øQu√© validaciones aplicas en `Pydantic`? (rango, tipos, campos extra)\n",
    "- ¬øC√≥mo garantizas el **orden** de columnas?\n",
    "- ¬øQu√© devuelves adem√°s de la predicci√≥n? (probabilidades, latencia, advertencias)\n",
    "\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ChurnFeatures(BaseModel):\n",
    "    Account_Length: int = Field(..., ge=0, le=300)\n",
    "    Area_Code: int = Field(..., ge=100, le=999)\n",
    "    Intl_Plan: int = Field(..., ge=0, le=1)\n",
    "    Vmail_Plan: int = Field(..., ge=0, le=1)\n",
    "    Vmail_Message: int = Field(..., ge=0, le=200)\n",
    "    CustServ_Calls: int = Field(..., ge=0, le=20)\n",
    "    Total_Calls: float = Field(..., ge=0, le=1000)\n",
    "    Total_Mins: float = Field(..., ge=0, le=2000)\n",
    "    Total_Charge: float = Field(..., ge=0, le=500)\n",
    "    High_Usage: int = Field(..., ge=0, le=1)\n",
    "    Many_CustServ_Calls: int = Field(..., ge=0, le=1)\n",
    "\n",
    "\n",
    "class ChurnResponse(BaseModel):\n",
    "    churn_prediction: int\n",
    "    churn_probability: float\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf840dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ChurnFeatures(BaseModel):\n",
    "    Account_Length: int = Field(..., ge=0, le=300)\n",
    "    Area_Code: int = Field(..., ge=100, le=999)\n",
    "    Intl_Plan: int = Field(..., ge=0, le=1)\n",
    "    Vmail_Plan: int = Field(..., ge=0, le=1)\n",
    "    Vmail_Message: int = Field(..., ge=0, le=200)\n",
    "    CustServ_Calls: int = Field(..., ge=0, le=20)\n",
    "    Total_Calls: float = Field(..., ge=0, le=1000)\n",
    "    Total_Mins: float = Field(..., ge=0, le=2000)\n",
    "    Total_Charge: float = Field(..., ge=0, le=500)\n",
    "    High_Usage: int = Field(..., ge=0, le=1)\n",
    "    Many_CustServ_Calls: int = Field(..., ge=0, le=1)\n",
    "\n",
    "class ChurnResponse(BaseModel):\n",
    "    churn_prediction: int\n",
    "    churn_probability: float\n",
    "    warnings : list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8abce9f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 4) Implementaci√≥n de FastAPI (investigaci√≥n)\n",
    "Crea un archivo `app.py` con:\n",
    "- Carga perezosa de `model.joblib` y `feature_columns.json`\n",
    "- Esquemas Pydantic (v2)\n",
    "- Endpoints `/health`, `/predict`, `/predict-batch`\n",
    "- Manejo de errores con `HTTPException` y mensajes claros\n",
    "\n",
    "**Pistas** (no copiar/pegar sin entender):\n",
    "- `model = joblib.load(...)`\n",
    "- `class Sample(BaseModel): ...`\n",
    "- `model.predict` y/o `model.predict_proba`\n",
    "- Retornar JSON con `dict | BaseModel`\n",
    "\n",
    "**Requisito:** esta celda **debe** escribir `app.py` con al menos la estructura b√°sica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41fccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from fastapi import FastAPI, Request, HTTPException\n",
    "from fastapi.responses import JSONResponse,FileResponse\n",
    "from src.schemas import ChurnFeatures, ChurnBatch\n",
    "from src.utils.validateDataEndpoint import validate_churn_features\n",
    "\n",
    "\n",
    "app = FastAPI(title=\"Churn Prediction API\")\n",
    "\n",
    "# Cargar modelo\n",
    "artifacts = joblib.load(\"models/trainModel.pkl\")\n",
    "model = artifacts[\"model\"]\n",
    "columns = artifacts[\"columns\"]\n",
    "\n",
    "# Configurar logger b√°sico\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Middleware de latencia y logger\n",
    "# -----------------------------\n",
    "@app.middleware(\"http\")\n",
    "async def add_process_time_header(request: Request, call_next):\n",
    "    start_time = time.time()\n",
    "    response = await call_next(request)\n",
    "    process_time_ms = (time.time() - start_time) * 1000\n",
    "    response.headers[\"X-Process-Time-ms\"] = f\"{process_time_ms:.2f}\"\n",
    "    \n",
    "    # Log estructurado\n",
    "    logging.info({\n",
    "        \"path\": request.url.path,\n",
    "        \"method\": request.method,\n",
    "        \"status_code\": response.status_code,\n",
    "        \"process_time_ms\": round(process_time_ms, 2)\n",
    "    })\n",
    "    return response\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health():\n",
    "    \"\"\"Endpoint de estado de la API\"\"\"\n",
    "    return {\"status\": \"ok\", \"model_loaded\": bool(model)}\n",
    "\n",
    "# -----------------------------\n",
    "# Endpoint de predicci√≥n\n",
    "# -----------------------------\n",
    "@app.post(\"/predict\")\n",
    "def predict(data: ChurnFeatures):\n",
    "    try:\n",
    "\n",
    "        validate_churn_features(data)\n",
    "\n",
    "        input_data = np.array([[ \n",
    "            data.Account_Length,\n",
    "            data.Area_Code,\n",
    "            data.Intl_Plan,\n",
    "            data.Vmail_Plan,\n",
    "            data.Vmail_Message,\n",
    "            data.CustServ_Calls,\n",
    "            data.Total_Calls,\n",
    "            data.Total_Mins,\n",
    "            data.Total_Charge,\n",
    "            data.High_Usage,\n",
    "            data.Many_CustServ_Calls\n",
    "        ]])\n",
    "\n",
    "        prediction = model.predict(input_data)[0]\n",
    "        probability = model.predict_proba(input_data)[:, 1][0]\n",
    "\n",
    "        return JSONResponse({\n",
    "            \"churn_prediction\": int(prediction),\n",
    "            \"churn_probability\": float(probability)\n",
    "        })\n",
    "\n",
    "    except HTTPException as e:\n",
    "        raise e\n",
    "\n",
    "    except Exception as e:\n",
    "        return JSONResponse(status_code=500, content={\"detail\": f\"Error al predecir: {str(e)}\"})\n",
    "\n",
    "\n",
    "@app.post(\"/predict_batch\")\n",
    "def predict_batch(batch_data: ChurnBatch):\n",
    "    \"\"\"Predicci√≥n para m√∫ltiples registros\"\"\"\n",
    "    try:\n",
    "        results = []\n",
    "        for record in batch_data.batch:\n",
    "            \n",
    "            validate_churn_features(record)\n",
    "\n",
    "            input_data = np.array([[\n",
    "                record.Account_Length,\n",
    "                record.Area_Code,\n",
    "                record.Intl_Plan,\n",
    "                record.Vmail_Plan,\n",
    "                record.Vmail_Message,\n",
    "                record.CustServ_Calls,\n",
    "                record.Total_Calls,\n",
    "                record.Total_Mins,\n",
    "                record.Total_Charge,\n",
    "                record.High_Usage,\n",
    "                record.Many_CustServ_Calls\n",
    "            ]])\n",
    "            prediction = model.predict(input_data)[0]\n",
    "            probability = model.predict_proba(input_data)[:, 1][0]\n",
    "            results.append({\n",
    "                \"prediction\": int(prediction),\n",
    "                \"probability\": float(probability)\n",
    "            })\n",
    "        return {\"results\": results}\n",
    "\n",
    "    except HTTPException as e:\n",
    "        raise e\n",
    "\n",
    "    except Exception as e:\n",
    "        return JSONResponse(status_code=500, content={\"detail\": f\"Error al predecir: {str(e)}\"})\n",
    "\n",
    "\n",
    "@app.get(\"/export_predictions\")\n",
    "def export_predictions():\n",
    "\n",
    "    \"\"\"Genera y devuelve un Excel con predicciones usando las columnas guardadas en el joblib.\"\"\"\n",
    "\n",
    "    df = pd.DataFrame(columns=artifacts[\"columns\"]) \n",
    "    df[\"predicted_churn\"] = model.predict(df)\n",
    "    df[\"probability_churn\"] = model.predict_proba(df)[:, 1]\n",
    "\n",
    "    output_path = \"predicciones_churn.xlsx\"\n",
    "    df.to_excel(output_path, index=False)\n",
    "\n",
    "    return FileResponse(\n",
    "        output_path,\n",
    "        filename=\"predicciones_churn.xlsx\",\n",
    "        media_type=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3effd9b0",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1 Ejecutar el servidor\n",
    "```bash\n",
    "uvicorn app:app --reload --port 8000\n",
    "```\n",
    "Abre `http://127.0.0.1:8000/docs` y prueba manualmente. Registra resultados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59771f1d",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 5) Pruebas y casos l√≠mite (investigaci√≥n)\n",
    "Define y ejecuta **al menos 6** pruebas:\n",
    "- 3 v√°lidas (predict y batch)\n",
    "- 3 inv√°lidas (campo faltante, tipo incorrecto, campo extra, etc.)\n",
    "\n",
    "Incluye c√≥digo de prueba y captura de respuestas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc5c4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "BASE_URL = \"http://127.0.0.1:8000\"\n",
    "\n",
    "# Cargar payloads desde archivo\n",
    "with open(\"./fixtures/payloads.json\", \"r\") as file:\n",
    "    payloads = json.load(file)\n",
    "\n",
    "# ----------------------------\n",
    "# Health check\n",
    "# ----------------------------\n",
    "response = requests.get(f\"{BASE_URL}/health\")\n",
    "print(\"\\n=== Health Check ===\")\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response:\", response.json())\n",
    "\n",
    "# ----------------------------\n",
    "# Predicci√≥n individual (tomamos el primer objeto del JSON)\n",
    "# ----------------------------\n",
    "first_payload = payloads[0]  # primer objeto del array de payloads\n",
    "\n",
    "response = requests.post(f\"{BASE_URL}/predict\", json=first_payload)\n",
    "print(\"\\n=== Individual Prediction ===\")\n",
    "print(\"Payload:\", json.dumps(first_payload, indent=4))\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response:\", response.json())\n",
    "\n",
    "# ----------------------------\n",
    "# Predicci√≥n batch (todos los objetos)\n",
    "# ----------------------------\n",
    "batch_payload = {\"batch\": payloads}\n",
    "response = requests.post(f\"{BASE_URL}/predict_batch\", json=batch_payload)\n",
    "print(\"\\n=== Batch Prediction ===\")\n",
    "print(\"Payload:\", json.dumps(batch_payload, indent=4))\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response:\", response.json())\n",
    "\n",
    "# ----------------------------\n",
    "# Exportar Excel\n",
    "# ----------------------------\n",
    "response = requests.get(f\"{BASE_URL}/export_predictions\")\n",
    "print(\"\\n=== Export Predictions ===\")\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Content-Disposition:\", response.headers.get(\"Content-Disposition\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662bd072",
   "metadata": {},
   "source": [
    "# Dockerfile\n",
    "# 1. Imagen base\n",
    "FROM python:3.11-slim\n",
    "\n",
    "# 2. Crear directorio de trabajo\n",
    "WORKDIR /app\n",
    "\n",
    "# 3. Copiar requirements y c√≥digo\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "COPY . .\n",
    "\n",
    "# 4. Exponer puerto y comando\n",
    "EXPOSE 8000\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0a67ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60d5fcca",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 7) Criterios de evaluaci√≥n (r√∫brica breve)\n",
    "- **Justificaci√≥n t√©cnica (25%)**: dataset, m√©trica, modelo y preprocesamiento argumentados.\n",
    "- **Calidad del pipeline (20%)**: reproducibilidad y limpieza (Pipeline, persistencia correcta).\n",
    "- **Contrato y validaciones (25%)**: Pydantic coherente, errores claros, orden de features garantizado.\n",
    "- **Pruebas (20%)**: variedad de casos, evidencia de resultados y manejo de fallos.\n",
    "- **C√≥digo y documentaci√≥n (10%)**: legibilidad, estructura y claridad de mensajes.\n",
    "\n",
    "---\n",
    "## Bit√°cora de decisiones (responde aqu√≠)\n",
    "- Dataset y objetivo:\n",
    "- Selecci√≥n de features/target:\n",
    "- Modelo y preprocesamiento:\n",
    "- M√©trica principal y resultados:\n",
    "- Decisiones de contrato (payload, validaciones, respuestas):\n",
    "- Observabilidad y pruebas:\n",
    "- Lecciones aprendidas:\n",
    "\n",
    "\n",
    "\n",
    "# Bit√°cora del Proyecto: Predicci√≥n de Churn\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Dataset y objetivo\n",
    "\n",
    "- **Dataset:** Informaci√≥n de clientes de telecomunicaciones, incluyendo caracter√≠sticas de uso (llamadas, minutos, cargos), planes contratados y llamadas al servicio de atenci√≥n al cliente.\n",
    "- **Objetivo:** Predecir si un cliente realizar√° churn (abandono del servicio) basado en sus patrones de uso y caracter√≠sticas del plan.\n",
    "- **Motivaci√≥n:** Identificar clientes con alto riesgo de churn permite tomar acciones preventivas para retenerlos.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Selecci√≥n de features/target\n",
    "\n",
    "- **Target:** `Churn` (0 = no churn, 1 = churn)\n",
    "- **Features originales:**  \n",
    "  - `Account_Length`, `Area_Code`, `Intl_Plan`, `Vmail_Plan`, `Vmail_Message`, `CustServ_Calls`, `Day_Mins`, `Eve_Mins`, `Night_Mins`, `Intl_Mins`, `Day_Calls`, `Eve_Calls`, `Night_Calls`, `Intl_Calls`, `Day_Charge`, `Eve_Charge`, `Night_Charge`, `Intl_Charge`\n",
    "- **Transformaciones y feature engineering:**\n",
    "  - Codificaci√≥n binaria de planes: `Intl_Plan`, `Vmail_Plan`\n",
    "  - Derivadas:  \n",
    "    - `Total_Calls` = suma de todas las llamadas  \n",
    "    - `Total_Mins` = suma de minutos totales  \n",
    "    - `Total_Charge` = suma de cargos totales  \n",
    "    - `High_Usage` = clientes con `Total_Charge` mayor que el promedio  \n",
    "    - `Many_CustServ_Calls` = clientes con m√°s de 5 llamadas a servicio\n",
    "- **Columnas finales:**  \n",
    "\n",
    "`['Account_Length', 'Area_Code', 'Intl_Plan', 'Vmail_Plan', 'Vmail_Message', 'CustServ_Calls',\n",
    "'Total_Calls', 'Total_Mins', 'Total_Charge', 'High_Usage', 'Many_CustServ_Calls']`\n",
    "\n",
    "\n",
    "## 3) Modelo y preprocesamiento\n",
    "\n",
    "- **Modelo:** Decision Tree Classifier (√°rbol de decisi√≥n)  \n",
    "- **Par√°metros ajustados:**  \n",
    "- `max_depth = 5` ‚Üí limita la complejidad para evitar overfitting  \n",
    "- `min_samples_split = 20` ‚Üí evita divisiones sobre muestras muy peque√±as  \n",
    "- `min_samples_leaf = 10` ‚Üí asegura que cada hoja tenga un m√≠nimo de registros  \n",
    "- **Preprocesamiento:**  \n",
    "- Transformaci√≥n de variables categ√≥ricas a num√©ricas\n",
    "- Generaci√≥n de features derivadas\n",
    "- Eliminaci√≥n de columnas redundantes\n",
    "- Eliminaci√≥n de valores nulos\n",
    "\n",
    "## 4) M√©trica principal y resultados\n",
    "\n",
    "- **M√©tricas obtenidas en test:**\n",
    "- Accuracy: 0.931 ‚Üí proporci√≥n total de predicciones correctas\n",
    "- Precision: 0.762 ‚Üí de los clientes predichos como churn, 76% realmente churn\n",
    "- Recall: 0.764 ‚Üí de los clientes que hicieron churn, 76% fueron detectados. Detectamos 3 de cada 4 clientes.\n",
    "- F1-score: 0.762 ‚Üí balance entre precisi√≥n y recall\n",
    "- **Interpretaci√≥n:**  \n",
    "- La m√©trica principal depende del objetivo de negocio. Para retenci√≥n, generalmente **recall** es clave: no perder clientes que realmente van a churn.\n",
    "- La precisi√≥n tambi√©n es importante para no gastar recursos en clientes que no van a churn.\n",
    "- Nuestro modelo muestra un buen balance entre precisi√≥n y recall.\n",
    "\n",
    "\n",
    "\n",
    "## 5) Decisiones de contrato (payload, validaciones, respuestas)\n",
    "\n",
    "- **Payload de entrada (`ChurnInput`):**\n",
    "```json\n",
    "{\n",
    "  \"Account_Length\": 150,\n",
    "  \"Area_Code\": 415,\n",
    "  \"Intl_Plan\": 1,\n",
    "  \"Vmail_Plan\": 0,\n",
    "  \"Vmail_Message\": 50,\n",
    "  \"CustServ_Calls\": 3,\n",
    "  \"Total_Calls\": 500,\n",
    "  \"Total_Mins\": 1200,\n",
    "  \"Total_Charge\": 250,\n",
    "  \"High_Usage\": 1,\n",
    "  \"Many_CustServ_Calls\": 0\n",
    "}```\n",
    "\n",
    "\n",
    "Validaciones:\n",
    "\n",
    "Rangos para cada campo\n",
    "Tipos num√©ricos\n",
    "Campos obligatorios\n",
    "\n",
    "```json response\n",
    "{\n",
    "  \"predicted_churn\": 1,\n",
    "  \"probability_churn\": 0.78,\n",
    "  \"warnings\": [\"CustServ_Calls fuera de rango t√≠pico\"]\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "## 6) Observabilidad y pruebas\n",
    "\n",
    "- Middleware de latencia: a√±ade header X-Process-Time-ms a cada respuesta\n",
    "- Logger estructurado: registro de cada request con ruta, m√©todo, status y tiempo de procesamiento\n",
    "- Pruebas realizadas:\n",
    "- Casos v√°lidos: 3 (diferentes perfiles de clientes)\n",
    "- Casos inv√°lidos: 3 (campo faltante, tipo incorrecto, campo extra)\n",
    "- Verificaci√≥n de probabilidades y predicciones\n",
    "- Endpoint adicional: /export_predictions\n",
    "- Genera un Excel con los datos originales, las features derivadas y las predicciones + probabilidades\n",
    "\n",
    "\n",
    "\n",
    "## 7) Lecciones aprendidas\n",
    "\n",
    "- La selecci√≥n de features derivadas (Total_Calls, Total_Charge, High_Usage) mejor√≥ el desempe√±o del modelo sin agregar complejidad innecesaria.\n",
    "- Ajustar min_samples_split y min_samples_leaf permite balancear overfitting y precisi√≥n.\n",
    "- Es cr√≠tico definir claramente el payload y validaciones para APIs de ML, evitando errores por desorden de columnas o tipos de datos.\n",
    "- Medir tanto precision como recall ayuda a decidir umbrales de acci√≥n para negocio.\n",
    "- Registrar m√©tricas de latencia y warnings incrementa la confiabilidad y explicabilidad de la API.\n",
    "- La documentaci√≥n clara y el pipeline reproducible facilitan la entrega y el entendimiento por terceros.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
